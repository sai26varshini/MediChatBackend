# ğŸ©º MediChat Backend (FastAPI + LangChain + FAISS)
This backend processes uploaded PDF medical reports, extracts text, embeds the content into a FAISS vector database, and answers user questions using an LLM.

---
## ğŸš€ Features
- Upload medical report PDFs
- Extracts text (supports scanned PDFs via PyMuPDF)
- Embeds & stores all PDF data in FAISS
- Asks natural-language questions using an LLM
- Stores knowledge across multiple uploads

---
## ğŸ› ï¸ Tech Stack
- FastAPI
- LangChain + HuggingFace Embeddings
- FAISS Vector Store
- PyMuPDF PDF Loader
- Python 3.11

---
## ğŸ“ Folder Structure
backend/
â”‚â”€â”€ main.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ app/
â”‚ â”œâ”€â”€ chat_utils.py
â”‚ â”œâ”€â”€ vectorstore_utils.py
â”‚â”€â”€ uploaded_pdfs/
â”‚â”€â”€ vectorstore/ (created automatically)


---
## â–¶ï¸ Setup

### 1ï¸âƒ£ Create environment
```bash
python -m venv medi
medi\Scripts\activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Optional: remove old knowledge
rmdir /s /q vectorstore

4ï¸âƒ£ Start server
uvicorn main:app --reload

Backend runs at:
http://127.0.0.1:8000

RESPONSE:-
POST
/upload_pdf
File Uploaded

POST
/chat
Example:-
JSON body:-
{
  "question": "What does my report say?"
}
